---
title: "GR5223\nTake-Home Final Exam"
author: 'Euiyoung Chung( UNI: ec3243 )'
date: "5/4/2018"
output: pdf_document
---

## Problem 1. A firm is attempting to evaluate the quality of its sales staff and is trying to find an examination or series of tests that may reveal the potential for good performance in sales.

### 1 - (a) Obtain the principal component solution for k = 3 common factors.

```{r}
# Import data
data <- read.csv("Salespeople.csv")

# Get correlation mat
R <- cor(data)

# Spectral decomposition
sp <- eigen(R)
gamma <- sp$vectors; lambda <- diag(sp$values)

# Let`s use 3 common factors
q_hat <- (gamma %*% sqrt(lambda))[,1:3]; q_hat
```

This is the principle component solution for $\hat{Q}$

```{r}
psi_hat <- diag(diag(R - q_hat %*% t(q_hat))); round(psi_hat,4)
```

This is the principle component solution for $\hat{\Psi}$


### 1 - (a) - (i) List the estimated specific variances

\[0.0454, 0.3052, 0.0122, 0.0330, 0.0386, 0.0127, 0.0866\]

### 1 - (a) - (ii) Obtain the (varimax) rotated loadings, and interpret the resulting factor solution.

```{r}
varimax(q_hat)$loadings
```

The first factor loadings strongly represent X4 and Y2 with the same sign for all the variables.   
The second factor loadings mostly represent X1 and X2.  
The third factor loadings mostly represent X3.  

### 1 - (b) Obtain the maximum likelihood solution for k = 3 common factors.

```{r}
mle <- factanal(~x1+x2+x3+x4+y1+y2+y3, data=data, factors=3, rotation="none");mle
```

See the loadings, that is, $\hat{Q}$ and the diagonal elements of $\hat{\Psi}$.

### 1 - (b) - (i) List the estimated specific variances

```{r}
round(mle$uniquenesses, 4)
```

\[0.0050, 0.4466, 0.0050, 0.0375, 0.0385, 0.0344, 0.0881\]

### 1 - (b) - (ii) Obtain the (varimax) rotated loadings, and interpret the resulting factor solution.

```{r}
mle_rot <- factanal(~x1+x2+x3+x4+y1+y2+y3, data=data, factors=3, rotation="varimax"); mle_rot$loadings
```

The first factor loadings strongly represent X4 and Y2.  
The second factor loadings mostly represents X1.  
The third factor loadings mostly represents X3.

### 1 - (c) Using the rotated principal component solution for k = 3, calculate this salespersonâ€™s factor scores by the regression method.

```{r}
fac_score <- factanal(~x1+x2+x3+x4+y1+y2+y3, data=data, factors=3, rotation="varimax", scores='regression')$scores
head(fac_score, 10)
```

### 1 - (d) Using the rotated maximum likelihood solution for k = 3, make a scatterplot matrix of the factor scores, calculated by the regression method, for the n = 50 observed cases.

```{r}
pairs(fac_score)
```

## Problem 2

```{r}
data <- read.csv("Irises.csv")
data$Species <- as.factor(data$Species)
```

### 2 - (a) Draw the scatterplot of (x1, x2) using different plotting symbols to indicate species

```{r}
library(ggplot2)
data_ji <- data.frame(apply(data[1:2], 2, jitter), Species = factor(data[,3]))
ggplot(data_ji, aes(x = x1, y = x2, shape=Species)) +
  geom_point()
```

The assumption of bivariate normal populations seem pretty reasonable based on the scatter plot for the sample points from the three populations.

### 2 - (b) Assuming the samples are from bivariate normal populations with a common covariance matrix, test the hypothese.

```{r}
library(dplyr)
# Get seperate matrices for population 2 and 3
sam2 <- data[data$Species == 2,] %>% 
  select(-Species)
sam3 <- data[data$Species == 3,] %>% 
  select(-Species)

# Sample size, parameter size
n2 <- nrow(sam2); n3 <- nrow(sam3)
p <- ncol(sam2)

# Get SD
S2 <- cov(sam2); S3 <- cov(sam3) 
S_pooled <- (1/(n2+n3-2))*((n2-1)*S2 + (n3-1)*S3)

# Get means
x_bar2 <- colMeans(sam2)
x_bar3 <- colMeans(sam3)

# Get T-squred
T2 <- 1/(1/n2 + 1/n3) * sum((x_bar2 - x_bar3) %*% solve(S_pooled, (x_bar2 - x_bar3)))

# F distribution
f_val <- T2 * (n2+n3-p-1)/((n2+n3-2)*p)

# p-value
p_val <- 1 - pf(f_val, p, n2+n3-p-1); p_val
```

See the p-value is extremly close to 0. This indicates that the null hypothesis, $H_0: \mu_{1} = \mu_{2}$ should be rejected.

```{r}
print(S2); print(S3)
```

The common covariance assumption seems fairly reasonable.

### 2 - (c) Estimate the posterior probabilities. 

```{r}
# New point for prediction
x_new <- c(3.5, 1.75)

# Prior distribution
prior <- c(1/3, 1/3, 1/3)

# Summary statistics
n1 <- sum(data$Species==1); n2 <- sum(data$Species==2); n3 <- sum(data$Species==3)

xbar1 <- colMeans(data[data$Species==1, 1:2])
xbar2 <- colMeans(data[data$Species==2, 1:2])
xbar3 <- colMeans(data[data$Species==3, 1:2])

S1 <- cov(data[data$Species==1, 1:2])
S2 <- cov(data[data$Species==2, 1:2])
S3 <- cov(data[data$Species==3, 1:2])

S <- 1/(n1+n2+n3-3) * ((n1-1)*S1 + (n2-1)*S2 + (n3-1)*S3)

# Calculate L(x)
xbar <- rbind(xbar1, xbar2, xbar3)
J <- 3
L <- rep(NA, J)

for (j in 1:J){
  L[j] <- log(prior[j]) + sum((x_new-xbar[j, ]/2)*solve(S, xbar[j,]))
}
L
```

See the value of L2 is the greatest. This indicates that the new point, (3.5, 1.75), should be classified as a sample from population 2 based on a linear classifier. 

```{r}
# Posterior probabilty
exp(L - max(L))/sum(exp(L-max(L)))
```

These are the posterior probabilities.  
$P(\Pi_{1}|x)$ = 3.20e-14  
$P(\Pi_{2}|x)$ = 7.18e-01  
$P(\Pi_{3}|x)$ = 2.81e-01

### 2 - (d) Evaluate the performance of the linear classifier with equal prior probabilities by computing the cross-validation estimate of the actual error rate.

```{r}
library(MASS)
holdout <- data$Species

for (ij in 1:length(holdout)){
  hold <- lda(Species ~ x1+x2, data=data[-ij,], prior=c(1/3,1/3,1/3))
  holdout[ij] <- predict(hold, newdata=data[ij,])$class
}

confusion <- xtabs(~data$Species + holdout)
error.cv <- 1 - sum(diag(confusion)) / sum(confusion)
error.cv
```

The estimate of the actual error rate is 0.04 based on cross-validation.

## Problem 3

```{r}
data <- read.csv("Bankruptcy.csv")
data$Population <- factor(data$Population)
```

### 3 - (a) Construct a scatterplot matrix. Which pair of variables seems to give the best separation between the two groups?

```{r}
pairs(data[,-5], pch=as.numeric(data$Population))
```





### 3 - (b) Assuming both random samples are from multivariate normal populations, estimate the posterior probability of bankruptcy for a firm.

```{r}
# Prior Probability
prior <- c(0.05, 0.95)
# Get summary statistics
n1 <- nrow(data[data$Population==1,]); n2 <- nrow(data[data$Population==2,])

S1 <- cov(data[data$Population==1,1:4])
S2 <- cov(data[data$Population==2,1:4])
S <- 1/(n1+n2-3) * ((n1-1)*S1 + (n2-1)*S2)

xbar1 <- colMeans(data[data$Population==1,1:4])
xbar2 <- colMeans(data[data$Population==2,1:4])

# Let`s get L(x)
xbar <- rbind(xbar1, xbar2)
J <- 2
L <- rep(NA, J)

for (j in 1:J){
  L[j] <- log(prior[j]) + sum((x_new-xbar[j, ]/2)*solve(S, xbar[j,]))
}

# Get posterior probabilities
exp(L - max(L))/sum(exp(L-max(L)))
```

The posterior probability of bankruptcy for a firm is 7.315746e-08.


### 3 - (c) Evaluate the classification rule used in part (b) by computing the apparent error rate, as well as the cross-validation estimate of the actual error rate.

```{r}
# Apparent error rate
lda_crude <- lda(Population ~ x1+x2+x3+x4, data=data, prior=prior)
confusion <- xtabs(~data$Population + predict(lda_crude)$class)
1 - sum(diag(confusion))/sum(confusion)
```

The apparent error rate is 0.391.

```{r}
# Cross-validation
holdout <- data$Population

for (ij in 1:length(holdout)){
  hold <- lda(Population ~ x1+x2+x3+x4, data=data[-ij,], prior=prior)
  holdout[ij] <- predict(hold, newdata=data[ij,])$class
}

confusion <- xtabs(~ data$Population + holdout)
1-sum(diag(confusion))/sum(confusion)
```

The cross-validation estimate of the actual error rate is 0.413.

## Problem 4.

### 4 - (a) Obtain the sample correlation matrix and determine its eigenvalues and eigenvectors. Inter- pret the first two normalized principal components.

```{r}
# Import data
data <- read.csv("Protein.csv")

# Correlation Matrix
R <- cor(data[,-1])

# Eigen values & vectors
gamma <- eigen(R)$vectors; rownames(gamma) <- colnames(data[,-1])
lambda <- eigen(R)$values

# Two normalized PC
gamma[,1:2]
```

The first principle component mostly represents the variance of Cereals(+), Nuts(+) and Eggs(-).  
The second principle component mostly represents the variance of Fish(-) and FruitVeg(-).


### 4 - (b) Compute the distance matrix. Produce a dendogram for the complete-linkage, agglomerative hierarchical clustering algorithm. Does this clustering make sense, given what you know about the countries?

```{r}
# Distance Matrix
D <- dist(scale(data[,-1]), method='euclidean'); round(D)
```

```{r}
# Dendogram
hc <- hclust(D, method="complete")
plot(hc, hang=-1, labels=data$Country)
rect.hclust(hc, h=4, border='red')
```

This clustering makes sense in that the countries in the same clusters are geologically close to each other. For instance Finland, Norway, Denmark, and Sweden are located in the nordic area together.

### 4 - (c) Make a scatterplot of the first two principal components, using different plotting symbols to indicate cluster membership. Briefly explain what this analysis tells us about the differences in diet across different regions of Europe.


```{r}
# Get PCs
pc <- as.matrix(data[,-1]) %*% gamma[,1:2]
rownames(pc) <- data$Country

# Get cluster membership
member <- cutree(hc, h=4)

# Create plot
ggplot(data = as.data.frame(pc, member), aes(x=V1, y=V2, shape=factor(member))) +
  geom_point(size=2) +
  guides(shape=guide_legend(title="Cluster")) +
  xlab("PC1") +
  ylab("PC2")
```

Cluster 1, Albania, Bulgaria, Romania, and Yugoslavia, has the greatest average of PC1 and PC2. This indicates that they consume more cereals, nuts and less eggs, fish, and fruitveg.  
In essence, the high average in PC1 is relevant to the high consumption of cereals, nuts, and less eggs. The low average in PC1 is because of the low consumption of cereals, nuts and more eggs. The high average in PC2 implies the low consumption of fish and fruitveg. Vice versa.






